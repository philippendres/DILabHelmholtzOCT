{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:16:09.112326500Z",
     "start_time": "2023-11-24T20:16:03.963736500Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:20:01.109548800Z",
     "start_time": "2023-11-24T20:20:01.096405Z"
    }
   },
   "id": "c9c6e8b18635ee4a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/13.1k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "865e744ee45f4faa9b030bc36bb74694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"mean_iou\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:16:15.059669500Z",
     "start_time": "2023-11-24T20:16:11.425204600Z"
    }
   },
   "id": "6804e9c029a24a03"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phili\\anaconda3\\envs\\DILab\\Lib\\site-packages\\datasets\\features\\image.py:332: UserWarning: Downcasting array dtype int32 to uint8 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from transformers import SamProcessor, SamModel\n",
    "from datasets import Dataset, Image\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataset\n",
    "from torch.optim import Adam\n",
    "import monai\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "#TODO: Try other alternatives as base models\n",
    "# alternatives: [\"facebook/sam-vit-base\", \"facebook/sam-vit-huge\", \"facebook/sam-vit-large\", \"wanglab/medsam-vit-base\"]\n",
    "base_model_name = \"facebook/sam-vit-base\"\n",
    "processor = SamProcessor.from_pretrained(base_model_name)\n",
    "model = SamModel.from_pretrained(base_model_name)\n",
    "\n",
    "def transform_image(img):\n",
    "    img = img.transpose(2,1,0)\n",
    "    image = np.expand_dims(img, axis=3)\n",
    "    image = np.repeat(255 - image, 3, axis=3)\n",
    "    return image\n",
    "\n",
    "def get_valid_idx(mask):\n",
    "    idx = []\n",
    "    for i in range(0,61):\n",
    "        temp = mask[:,:,i]\n",
    "        if np.nansum(temp) != 0:\n",
    "            idx.append(i)\n",
    "    return idx\n",
    "\n",
    "def create_dataset(images, labels):\n",
    "    dataset = Dataset.from_dict({\"image\": images,\n",
    "                                 \"label\": labels})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def transforms(examples):\n",
    "    transformed_images, transformed_masks = [], []\n",
    "    for image, seg_mask in zip(examples[\"image\"], examples[\"label\"]):\n",
    "        image, seg_mask = np.array(image), np.array(seg_mask)\n",
    "        transformed = transform(image=image, mask=seg_mask)\n",
    "        transformed_images.append(transformed[\"image\"])\n",
    "        transformed_masks.append(transformed[\"mask\"])\n",
    "    examples[\"pixel_values\"] = transformed_images\n",
    "    examples[\"label\"] = transformed_masks\n",
    "    return examples\n",
    "\n",
    "def visualize_seg_mask(image: np.ndarray, mask: np.ndarray):\n",
    "    color_seg = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    color_seg[mask == 0, :] = [255,255,255]\n",
    "    #color_seg[mask == 1, :] = [255,255,255]\n",
    "    color_seg[mask == 1, :] = [255,0,0]\n",
    "    color_seg = color_seg[..., ::-1]  # convert to BGR\n",
    "    img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
    "    img = img.astype(np.uint8)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    # get bounding box from mask\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # add perturbation to bounding box coordinates\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return bbox\n",
    "\n",
    "class SAMDataset(TorchDataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"pixel_values\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        # get bounding box prompt\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        # prepare image and prompt for the model\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "        return inputs\n",
    "\n",
    "#make sure we only compute gradients for mask decoder\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "mat_dme =[]\n",
    "for i in range(10):\n",
    "    number = str(i+1).zfill(2)\n",
    "    mat_dme.append(\"..\\data\\Kaggle\\Subject_\"+number+\".mat\")\n",
    "\n",
    "\n",
    "#TODO: Extend Code to multiple subjects\n",
    "nr = 0\n",
    "images = loadmat(mat_dme[nr])['images']\n",
    "#TODO: Extend Code to manualFluid2\n",
    "masks = loadmat(mat_dme[nr])['manualFluid1']\n",
    "idx = get_valid_idx(masks)\n",
    "masks = masks[:,:, idx]\n",
    "images = images[:,:,idx]\n",
    "images = transform_image(images)\n",
    "#TODO: Extend Code to individual fluids (values 1,.. in the mask)\n",
    "masks = np.where(masks == 0, 0, 1).transpose(2,1,0)\n",
    "dataset = create_dataset(images=images, labels=masks)\n",
    "\n",
    "#TODO: Explore Options for transformations\n",
    "transform = albumentations.Compose([albumentations.Resize(256, 256),])\n",
    "dataset.set_transform(transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:24:18.687290800Z",
     "start_time": "2023-11-24T20:23:50.693657900Z"
    }
   },
   "id": "e953947bcb48cabe"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "base_model_name = \"facebook/sam-vit-base\"\n",
    "processor = SamProcessor.from_pretrained(base_model_name)\n",
    "model = SamModel.from_pretrained(base_model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:24:30.813632700Z",
     "start_time": "2023-11-24T20:24:29.499080800Z"
    }
   },
   "id": "7423cb38a0d2a16b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "states = torch.load(\"../data/Kaggle/model_checkpoints/chkpt1.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:31:16.695032800Z",
     "start_time": "2023-11-24T20:31:16.237973800Z"
    }
   },
   "id": "ccfa4e0e7c356232"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "_IncompatibleKeys(missing_keys=['shared_image_embedding.positional_embedding', 'vision_encoder.pos_embed', 'vision_encoder.patch_embed.projection.weight', 'vision_encoder.patch_embed.projection.bias', 'vision_encoder.layers.0.layer_norm1.weight', 'vision_encoder.layers.0.layer_norm1.bias', 'vision_encoder.layers.0.attn.rel_pos_h', 'vision_encoder.layers.0.attn.rel_pos_w', 'vision_encoder.layers.0.attn.qkv.weight', 'vision_encoder.layers.0.attn.qkv.bias', 'vision_encoder.layers.0.attn.proj.weight', 'vision_encoder.layers.0.attn.proj.bias', 'vision_encoder.layers.0.layer_norm2.weight', 'vision_encoder.layers.0.layer_norm2.bias', 'vision_encoder.layers.0.mlp.lin1.weight', 'vision_encoder.layers.0.mlp.lin1.bias', 'vision_encoder.layers.0.mlp.lin2.weight', 'vision_encoder.layers.0.mlp.lin2.bias', 'vision_encoder.layers.1.layer_norm1.weight', 'vision_encoder.layers.1.layer_norm1.bias', 'vision_encoder.layers.1.attn.rel_pos_h', 'vision_encoder.layers.1.attn.rel_pos_w', 'vision_encoder.layers.1.attn.qkv.weight', 'vision_encoder.layers.1.attn.qkv.bias', 'vision_encoder.layers.1.attn.proj.weight', 'vision_encoder.layers.1.attn.proj.bias', 'vision_encoder.layers.1.layer_norm2.weight', 'vision_encoder.layers.1.layer_norm2.bias', 'vision_encoder.layers.1.mlp.lin1.weight', 'vision_encoder.layers.1.mlp.lin1.bias', 'vision_encoder.layers.1.mlp.lin2.weight', 'vision_encoder.layers.1.mlp.lin2.bias', 'vision_encoder.layers.2.layer_norm1.weight', 'vision_encoder.layers.2.layer_norm1.bias', 'vision_encoder.layers.2.attn.rel_pos_h', 'vision_encoder.layers.2.attn.rel_pos_w', 'vision_encoder.layers.2.attn.qkv.weight', 'vision_encoder.layers.2.attn.qkv.bias', 'vision_encoder.layers.2.attn.proj.weight', 'vision_encoder.layers.2.attn.proj.bias', 'vision_encoder.layers.2.layer_norm2.weight', 'vision_encoder.layers.2.layer_norm2.bias', 'vision_encoder.layers.2.mlp.lin1.weight', 'vision_encoder.layers.2.mlp.lin1.bias', 'vision_encoder.layers.2.mlp.lin2.weight', 'vision_encoder.layers.2.mlp.lin2.bias', 'vision_encoder.layers.3.layer_norm1.weight', 'vision_encoder.layers.3.layer_norm1.bias', 'vision_encoder.layers.3.attn.rel_pos_h', 'vision_encoder.layers.3.attn.rel_pos_w', 'vision_encoder.layers.3.attn.qkv.weight', 'vision_encoder.layers.3.attn.qkv.bias', 'vision_encoder.layers.3.attn.proj.weight', 'vision_encoder.layers.3.attn.proj.bias', 'vision_encoder.layers.3.layer_norm2.weight', 'vision_encoder.layers.3.layer_norm2.bias', 'vision_encoder.layers.3.mlp.lin1.weight', 'vision_encoder.layers.3.mlp.lin1.bias', 'vision_encoder.layers.3.mlp.lin2.weight', 'vision_encoder.layers.3.mlp.lin2.bias', 'vision_encoder.layers.4.layer_norm1.weight', 'vision_encoder.layers.4.layer_norm1.bias', 'vision_encoder.layers.4.attn.rel_pos_h', 'vision_encoder.layers.4.attn.rel_pos_w', 'vision_encoder.layers.4.attn.qkv.weight', 'vision_encoder.layers.4.attn.qkv.bias', 'vision_encoder.layers.4.attn.proj.weight', 'vision_encoder.layers.4.attn.proj.bias', 'vision_encoder.layers.4.layer_norm2.weight', 'vision_encoder.layers.4.layer_norm2.bias', 'vision_encoder.layers.4.mlp.lin1.weight', 'vision_encoder.layers.4.mlp.lin1.bias', 'vision_encoder.layers.4.mlp.lin2.weight', 'vision_encoder.layers.4.mlp.lin2.bias', 'vision_encoder.layers.5.layer_norm1.weight', 'vision_encoder.layers.5.layer_norm1.bias', 'vision_encoder.layers.5.attn.rel_pos_h', 'vision_encoder.layers.5.attn.rel_pos_w', 'vision_encoder.layers.5.attn.qkv.weight', 'vision_encoder.layers.5.attn.qkv.bias', 'vision_encoder.layers.5.attn.proj.weight', 'vision_encoder.layers.5.attn.proj.bias', 'vision_encoder.layers.5.layer_norm2.weight', 'vision_encoder.layers.5.layer_norm2.bias', 'vision_encoder.layers.5.mlp.lin1.weight', 'vision_encoder.layers.5.mlp.lin1.bias', 'vision_encoder.layers.5.mlp.lin2.weight', 'vision_encoder.layers.5.mlp.lin2.bias', 'vision_encoder.layers.6.layer_norm1.weight', 'vision_encoder.layers.6.layer_norm1.bias', 'vision_encoder.layers.6.attn.rel_pos_h', 'vision_encoder.layers.6.attn.rel_pos_w', 'vision_encoder.layers.6.attn.qkv.weight', 'vision_encoder.layers.6.attn.qkv.bias', 'vision_encoder.layers.6.attn.proj.weight', 'vision_encoder.layers.6.attn.proj.bias', 'vision_encoder.layers.6.layer_norm2.weight', 'vision_encoder.layers.6.layer_norm2.bias', 'vision_encoder.layers.6.mlp.lin1.weight', 'vision_encoder.layers.6.mlp.lin1.bias', 'vision_encoder.layers.6.mlp.lin2.weight', 'vision_encoder.layers.6.mlp.lin2.bias', 'vision_encoder.layers.7.layer_norm1.weight', 'vision_encoder.layers.7.layer_norm1.bias', 'vision_encoder.layers.7.attn.rel_pos_h', 'vision_encoder.layers.7.attn.rel_pos_w', 'vision_encoder.layers.7.attn.qkv.weight', 'vision_encoder.layers.7.attn.qkv.bias', 'vision_encoder.layers.7.attn.proj.weight', 'vision_encoder.layers.7.attn.proj.bias', 'vision_encoder.layers.7.layer_norm2.weight', 'vision_encoder.layers.7.layer_norm2.bias', 'vision_encoder.layers.7.mlp.lin1.weight', 'vision_encoder.layers.7.mlp.lin1.bias', 'vision_encoder.layers.7.mlp.lin2.weight', 'vision_encoder.layers.7.mlp.lin2.bias', 'vision_encoder.layers.8.layer_norm1.weight', 'vision_encoder.layers.8.layer_norm1.bias', 'vision_encoder.layers.8.attn.rel_pos_h', 'vision_encoder.layers.8.attn.rel_pos_w', 'vision_encoder.layers.8.attn.qkv.weight', 'vision_encoder.layers.8.attn.qkv.bias', 'vision_encoder.layers.8.attn.proj.weight', 'vision_encoder.layers.8.attn.proj.bias', 'vision_encoder.layers.8.layer_norm2.weight', 'vision_encoder.layers.8.layer_norm2.bias', 'vision_encoder.layers.8.mlp.lin1.weight', 'vision_encoder.layers.8.mlp.lin1.bias', 'vision_encoder.layers.8.mlp.lin2.weight', 'vision_encoder.layers.8.mlp.lin2.bias', 'vision_encoder.layers.9.layer_norm1.weight', 'vision_encoder.layers.9.layer_norm1.bias', 'vision_encoder.layers.9.attn.rel_pos_h', 'vision_encoder.layers.9.attn.rel_pos_w', 'vision_encoder.layers.9.attn.qkv.weight', 'vision_encoder.layers.9.attn.qkv.bias', 'vision_encoder.layers.9.attn.proj.weight', 'vision_encoder.layers.9.attn.proj.bias', 'vision_encoder.layers.9.layer_norm2.weight', 'vision_encoder.layers.9.layer_norm2.bias', 'vision_encoder.layers.9.mlp.lin1.weight', 'vision_encoder.layers.9.mlp.lin1.bias', 'vision_encoder.layers.9.mlp.lin2.weight', 'vision_encoder.layers.9.mlp.lin2.bias', 'vision_encoder.layers.10.layer_norm1.weight', 'vision_encoder.layers.10.layer_norm1.bias', 'vision_encoder.layers.10.attn.rel_pos_h', 'vision_encoder.layers.10.attn.rel_pos_w', 'vision_encoder.layers.10.attn.qkv.weight', 'vision_encoder.layers.10.attn.qkv.bias', 'vision_encoder.layers.10.attn.proj.weight', 'vision_encoder.layers.10.attn.proj.bias', 'vision_encoder.layers.10.layer_norm2.weight', 'vision_encoder.layers.10.layer_norm2.bias', 'vision_encoder.layers.10.mlp.lin1.weight', 'vision_encoder.layers.10.mlp.lin1.bias', 'vision_encoder.layers.10.mlp.lin2.weight', 'vision_encoder.layers.10.mlp.lin2.bias', 'vision_encoder.layers.11.layer_norm1.weight', 'vision_encoder.layers.11.layer_norm1.bias', 'vision_encoder.layers.11.attn.rel_pos_h', 'vision_encoder.layers.11.attn.rel_pos_w', 'vision_encoder.layers.11.attn.qkv.weight', 'vision_encoder.layers.11.attn.qkv.bias', 'vision_encoder.layers.11.attn.proj.weight', 'vision_encoder.layers.11.attn.proj.bias', 'vision_encoder.layers.11.layer_norm2.weight', 'vision_encoder.layers.11.layer_norm2.bias', 'vision_encoder.layers.11.mlp.lin1.weight', 'vision_encoder.layers.11.mlp.lin1.bias', 'vision_encoder.layers.11.mlp.lin2.weight', 'vision_encoder.layers.11.mlp.lin2.bias', 'vision_encoder.neck.conv1.weight', 'vision_encoder.neck.layer_norm1.weight', 'vision_encoder.neck.layer_norm1.bias', 'vision_encoder.neck.conv2.weight', 'vision_encoder.neck.layer_norm2.weight', 'vision_encoder.neck.layer_norm2.bias', 'prompt_encoder.shared_embedding.positional_embedding', 'prompt_encoder.mask_embed.conv1.weight', 'prompt_encoder.mask_embed.conv1.bias', 'prompt_encoder.mask_embed.conv2.weight', 'prompt_encoder.mask_embed.conv2.bias', 'prompt_encoder.mask_embed.conv3.weight', 'prompt_encoder.mask_embed.conv3.bias', 'prompt_encoder.mask_embed.layer_norm1.weight', 'prompt_encoder.mask_embed.layer_norm1.bias', 'prompt_encoder.mask_embed.layer_norm2.weight', 'prompt_encoder.mask_embed.layer_norm2.bias', 'prompt_encoder.point_embed.0.weight', 'prompt_encoder.point_embed.1.weight', 'prompt_encoder.point_embed.2.weight', 'prompt_encoder.point_embed.3.weight', 'mask_decoder.transformer.layers.0.layer_norm1.weight', 'mask_decoder.transformer.layers.0.layer_norm1.bias', 'mask_decoder.transformer.layers.0.layer_norm2.weight', 'mask_decoder.transformer.layers.0.layer_norm2.bias', 'mask_decoder.transformer.layers.0.layer_norm3.weight', 'mask_decoder.transformer.layers.0.layer_norm3.bias', 'mask_decoder.transformer.layers.0.layer_norm4.weight', 'mask_decoder.transformer.layers.0.layer_norm4.bias', 'mask_decoder.transformer.layers.1.layer_norm1.weight', 'mask_decoder.transformer.layers.1.layer_norm1.bias', 'mask_decoder.transformer.layers.1.layer_norm2.weight', 'mask_decoder.transformer.layers.1.layer_norm2.bias', 'mask_decoder.transformer.layers.1.layer_norm3.weight', 'mask_decoder.transformer.layers.1.layer_norm3.bias', 'mask_decoder.transformer.layers.1.layer_norm4.weight', 'mask_decoder.transformer.layers.1.layer_norm4.bias', 'mask_decoder.transformer.layer_norm_final_attn.weight', 'mask_decoder.transformer.layer_norm_final_attn.bias', 'mask_decoder.upscale_conv1.weight', 'mask_decoder.upscale_conv1.bias', 'mask_decoder.upscale_conv2.weight', 'mask_decoder.upscale_conv2.bias', 'mask_decoder.upscale_layer_norm.weight', 'mask_decoder.upscale_layer_norm.bias', 'mask_decoder.output_hypernetworks_mlps.0.proj_in.weight', 'mask_decoder.output_hypernetworks_mlps.0.proj_in.bias', 'mask_decoder.output_hypernetworks_mlps.0.proj_out.weight', 'mask_decoder.output_hypernetworks_mlps.0.proj_out.bias', 'mask_decoder.output_hypernetworks_mlps.1.proj_in.weight', 'mask_decoder.output_hypernetworks_mlps.1.proj_in.bias', 'mask_decoder.output_hypernetworks_mlps.1.proj_out.weight', 'mask_decoder.output_hypernetworks_mlps.1.proj_out.bias', 'mask_decoder.output_hypernetworks_mlps.2.proj_in.weight', 'mask_decoder.output_hypernetworks_mlps.2.proj_in.bias', 'mask_decoder.output_hypernetworks_mlps.2.proj_out.weight', 'mask_decoder.output_hypernetworks_mlps.2.proj_out.bias', 'mask_decoder.output_hypernetworks_mlps.3.proj_in.weight', 'mask_decoder.output_hypernetworks_mlps.3.proj_in.bias', 'mask_decoder.output_hypernetworks_mlps.3.proj_out.weight', 'mask_decoder.output_hypernetworks_mlps.3.proj_out.bias', 'mask_decoder.iou_prediction_head.proj_in.weight', 'mask_decoder.iou_prediction_head.proj_in.bias', 'mask_decoder.iou_prediction_head.proj_out.weight', 'mask_decoder.iou_prediction_head.proj_out.bias'], unexpected_keys=['image_encoder.pos_embed', 'image_encoder.patch_embed.proj.weight', 'image_encoder.patch_embed.proj.bias', 'image_encoder.blocks.0.norm1.weight', 'image_encoder.blocks.0.norm1.bias', 'image_encoder.blocks.0.attn.rel_pos_h', 'image_encoder.blocks.0.attn.rel_pos_w', 'image_encoder.blocks.0.attn.qkv.weight', 'image_encoder.blocks.0.attn.qkv.bias', 'image_encoder.blocks.0.attn.proj.weight', 'image_encoder.blocks.0.attn.proj.bias', 'image_encoder.blocks.0.norm2.weight', 'image_encoder.blocks.0.norm2.bias', 'image_encoder.blocks.0.mlp.lin1.weight', 'image_encoder.blocks.0.mlp.lin1.bias', 'image_encoder.blocks.0.mlp.lin2.weight', 'image_encoder.blocks.0.mlp.lin2.bias', 'image_encoder.blocks.1.norm1.weight', 'image_encoder.blocks.1.norm1.bias', 'image_encoder.blocks.1.attn.rel_pos_h', 'image_encoder.blocks.1.attn.rel_pos_w', 'image_encoder.blocks.1.attn.qkv.weight', 'image_encoder.blocks.1.attn.qkv.bias', 'image_encoder.blocks.1.attn.proj.weight', 'image_encoder.blocks.1.attn.proj.bias', 'image_encoder.blocks.1.norm2.weight', 'image_encoder.blocks.1.norm2.bias', 'image_encoder.blocks.1.mlp.lin1.weight', 'image_encoder.blocks.1.mlp.lin1.bias', 'image_encoder.blocks.1.mlp.lin2.weight', 'image_encoder.blocks.1.mlp.lin2.bias', 'image_encoder.blocks.2.norm1.weight', 'image_encoder.blocks.2.norm1.bias', 'image_encoder.blocks.2.attn.rel_pos_h', 'image_encoder.blocks.2.attn.rel_pos_w', 'image_encoder.blocks.2.attn.qkv.weight', 'image_encoder.blocks.2.attn.qkv.bias', 'image_encoder.blocks.2.attn.proj.weight', 'image_encoder.blocks.2.attn.proj.bias', 'image_encoder.blocks.2.norm2.weight', 'image_encoder.blocks.2.norm2.bias', 'image_encoder.blocks.2.mlp.lin1.weight', 'image_encoder.blocks.2.mlp.lin1.bias', 'image_encoder.blocks.2.mlp.lin2.weight', 'image_encoder.blocks.2.mlp.lin2.bias', 'image_encoder.blocks.3.norm1.weight', 'image_encoder.blocks.3.norm1.bias', 'image_encoder.blocks.3.attn.rel_pos_h', 'image_encoder.blocks.3.attn.rel_pos_w', 'image_encoder.blocks.3.attn.qkv.weight', 'image_encoder.blocks.3.attn.qkv.bias', 'image_encoder.blocks.3.attn.proj.weight', 'image_encoder.blocks.3.attn.proj.bias', 'image_encoder.blocks.3.norm2.weight', 'image_encoder.blocks.3.norm2.bias', 'image_encoder.blocks.3.mlp.lin1.weight', 'image_encoder.blocks.3.mlp.lin1.bias', 'image_encoder.blocks.3.mlp.lin2.weight', 'image_encoder.blocks.3.mlp.lin2.bias', 'image_encoder.blocks.4.norm1.weight', 'image_encoder.blocks.4.norm1.bias', 'image_encoder.blocks.4.attn.rel_pos_h', 'image_encoder.blocks.4.attn.rel_pos_w', 'image_encoder.blocks.4.attn.qkv.weight', 'image_encoder.blocks.4.attn.qkv.bias', 'image_encoder.blocks.4.attn.proj.weight', 'image_encoder.blocks.4.attn.proj.bias', 'image_encoder.blocks.4.norm2.weight', 'image_encoder.blocks.4.norm2.bias', 'image_encoder.blocks.4.mlp.lin1.weight', 'image_encoder.blocks.4.mlp.lin1.bias', 'image_encoder.blocks.4.mlp.lin2.weight', 'image_encoder.blocks.4.mlp.lin2.bias', 'image_encoder.blocks.5.norm1.weight', 'image_encoder.blocks.5.norm1.bias', 'image_encoder.blocks.5.attn.rel_pos_h', 'image_encoder.blocks.5.attn.rel_pos_w', 'image_encoder.blocks.5.attn.qkv.weight', 'image_encoder.blocks.5.attn.qkv.bias', 'image_encoder.blocks.5.attn.proj.weight', 'image_encoder.blocks.5.attn.proj.bias', 'image_encoder.blocks.5.norm2.weight', 'image_encoder.blocks.5.norm2.bias', 'image_encoder.blocks.5.mlp.lin1.weight', 'image_encoder.blocks.5.mlp.lin1.bias', 'image_encoder.blocks.5.mlp.lin2.weight', 'image_encoder.blocks.5.mlp.lin2.bias', 'image_encoder.blocks.6.norm1.weight', 'image_encoder.blocks.6.norm1.bias', 'image_encoder.blocks.6.attn.rel_pos_h', 'image_encoder.blocks.6.attn.rel_pos_w', 'image_encoder.blocks.6.attn.qkv.weight', 'image_encoder.blocks.6.attn.qkv.bias', 'image_encoder.blocks.6.attn.proj.weight', 'image_encoder.blocks.6.attn.proj.bias', 'image_encoder.blocks.6.norm2.weight', 'image_encoder.blocks.6.norm2.bias', 'image_encoder.blocks.6.mlp.lin1.weight', 'image_encoder.blocks.6.mlp.lin1.bias', 'image_encoder.blocks.6.mlp.lin2.weight', 'image_encoder.blocks.6.mlp.lin2.bias', 'image_encoder.blocks.7.norm1.weight', 'image_encoder.blocks.7.norm1.bias', 'image_encoder.blocks.7.attn.rel_pos_h', 'image_encoder.blocks.7.attn.rel_pos_w', 'image_encoder.blocks.7.attn.qkv.weight', 'image_encoder.blocks.7.attn.qkv.bias', 'image_encoder.blocks.7.attn.proj.weight', 'image_encoder.blocks.7.attn.proj.bias', 'image_encoder.blocks.7.norm2.weight', 'image_encoder.blocks.7.norm2.bias', 'image_encoder.blocks.7.mlp.lin1.weight', 'image_encoder.blocks.7.mlp.lin1.bias', 'image_encoder.blocks.7.mlp.lin2.weight', 'image_encoder.blocks.7.mlp.lin2.bias', 'image_encoder.blocks.8.norm1.weight', 'image_encoder.blocks.8.norm1.bias', 'image_encoder.blocks.8.attn.rel_pos_h', 'image_encoder.blocks.8.attn.rel_pos_w', 'image_encoder.blocks.8.attn.qkv.weight', 'image_encoder.blocks.8.attn.qkv.bias', 'image_encoder.blocks.8.attn.proj.weight', 'image_encoder.blocks.8.attn.proj.bias', 'image_encoder.blocks.8.norm2.weight', 'image_encoder.blocks.8.norm2.bias', 'image_encoder.blocks.8.mlp.lin1.weight', 'image_encoder.blocks.8.mlp.lin1.bias', 'image_encoder.blocks.8.mlp.lin2.weight', 'image_encoder.blocks.8.mlp.lin2.bias', 'image_encoder.blocks.9.norm1.weight', 'image_encoder.blocks.9.norm1.bias', 'image_encoder.blocks.9.attn.rel_pos_h', 'image_encoder.blocks.9.attn.rel_pos_w', 'image_encoder.blocks.9.attn.qkv.weight', 'image_encoder.blocks.9.attn.qkv.bias', 'image_encoder.blocks.9.attn.proj.weight', 'image_encoder.blocks.9.attn.proj.bias', 'image_encoder.blocks.9.norm2.weight', 'image_encoder.blocks.9.norm2.bias', 'image_encoder.blocks.9.mlp.lin1.weight', 'image_encoder.blocks.9.mlp.lin1.bias', 'image_encoder.blocks.9.mlp.lin2.weight', 'image_encoder.blocks.9.mlp.lin2.bias', 'image_encoder.blocks.10.norm1.weight', 'image_encoder.blocks.10.norm1.bias', 'image_encoder.blocks.10.attn.rel_pos_h', 'image_encoder.blocks.10.attn.rel_pos_w', 'image_encoder.blocks.10.attn.qkv.weight', 'image_encoder.blocks.10.attn.qkv.bias', 'image_encoder.blocks.10.attn.proj.weight', 'image_encoder.blocks.10.attn.proj.bias', 'image_encoder.blocks.10.norm2.weight', 'image_encoder.blocks.10.norm2.bias', 'image_encoder.blocks.10.mlp.lin1.weight', 'image_encoder.blocks.10.mlp.lin1.bias', 'image_encoder.blocks.10.mlp.lin2.weight', 'image_encoder.blocks.10.mlp.lin2.bias', 'image_encoder.blocks.11.norm1.weight', 'image_encoder.blocks.11.norm1.bias', 'image_encoder.blocks.11.attn.rel_pos_h', 'image_encoder.blocks.11.attn.rel_pos_w', 'image_encoder.blocks.11.attn.qkv.weight', 'image_encoder.blocks.11.attn.qkv.bias', 'image_encoder.blocks.11.attn.proj.weight', 'image_encoder.blocks.11.attn.proj.bias', 'image_encoder.blocks.11.norm2.weight', 'image_encoder.blocks.11.norm2.bias', 'image_encoder.blocks.11.mlp.lin1.weight', 'image_encoder.blocks.11.mlp.lin1.bias', 'image_encoder.blocks.11.mlp.lin2.weight', 'image_encoder.blocks.11.mlp.lin2.bias', 'image_encoder.neck.0.weight', 'image_encoder.neck.1.weight', 'image_encoder.neck.1.bias', 'image_encoder.neck.2.weight', 'image_encoder.neck.3.weight', 'image_encoder.neck.3.bias', 'prompt_encoder.pe_layer.positional_encoding_gaussian_matrix', 'prompt_encoder.point_embeddings.0.weight', 'prompt_encoder.point_embeddings.1.weight', 'prompt_encoder.point_embeddings.2.weight', 'prompt_encoder.point_embeddings.3.weight', 'prompt_encoder.mask_downscaling.0.weight', 'prompt_encoder.mask_downscaling.0.bias', 'prompt_encoder.mask_downscaling.1.weight', 'prompt_encoder.mask_downscaling.1.bias', 'prompt_encoder.mask_downscaling.3.weight', 'prompt_encoder.mask_downscaling.3.bias', 'prompt_encoder.mask_downscaling.4.weight', 'prompt_encoder.mask_downscaling.4.bias', 'prompt_encoder.mask_downscaling.6.weight', 'prompt_encoder.mask_downscaling.6.bias', 'mask_decoder.output_upscaling.0.weight', 'mask_decoder.output_upscaling.0.bias', 'mask_decoder.output_upscaling.1.weight', 'mask_decoder.output_upscaling.1.bias', 'mask_decoder.output_upscaling.3.weight', 'mask_decoder.output_upscaling.3.bias', 'mask_decoder.transformer.norm_final_attn.weight', 'mask_decoder.transformer.norm_final_attn.bias', 'mask_decoder.transformer.layers.0.norm1.weight', 'mask_decoder.transformer.layers.0.norm1.bias', 'mask_decoder.transformer.layers.0.norm2.weight', 'mask_decoder.transformer.layers.0.norm2.bias', 'mask_decoder.transformer.layers.0.norm3.weight', 'mask_decoder.transformer.layers.0.norm3.bias', 'mask_decoder.transformer.layers.0.norm4.weight', 'mask_decoder.transformer.layers.0.norm4.bias', 'mask_decoder.transformer.layers.1.norm1.weight', 'mask_decoder.transformer.layers.1.norm1.bias', 'mask_decoder.transformer.layers.1.norm2.weight', 'mask_decoder.transformer.layers.1.norm2.bias', 'mask_decoder.transformer.layers.1.norm3.weight', 'mask_decoder.transformer.layers.1.norm3.bias', 'mask_decoder.transformer.layers.1.norm4.weight', 'mask_decoder.transformer.layers.1.norm4.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.0.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.1.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.2.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.1.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'mask_decoder.output_hypernetworks_mlps.3.layers.2.weight', 'mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'mask_decoder.iou_prediction_head.layers.1.weight', 'mask_decoder.iou_prediction_head.layers.1.bias', 'mask_decoder.iou_prediction_head.layers.2.weight', 'mask_decoder.iou_prediction_head.layers.2.bias'])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(states, strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:33:15.473204300Z",
     "start_time": "2023-11-24T20:33:15.428515600Z"
    }
   },
   "id": "846f46a09cba3be"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model.load_state_dict(states, strict=False)\n",
    "device = \"cuda\"\n",
    "idx = 3\n",
    "image = dataset[idx][\"pixel_values\"]\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, multimask_output=False)\n",
    "# apply sigmoid\n",
    "finetune_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "finetune_seg_prob = finetune_seg_prob.cpu().numpy().squeeze()\n",
    "finetune_seg = (finetune_seg_prob > 0.5).astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:42:04.758334600Z",
     "start_time": "2023-11-24T20:42:04.103893Z"
    }
   },
   "id": "302f047c79479cb4"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "base_model_name = \"facebook/sam-vit-base\"\n",
    "processor = SamProcessor.from_pretrained(base_model_name)\n",
    "model = SamModel.from_pretrained(base_model_name)\n",
    "device = \"cuda\"\n",
    "idx = 3\n",
    "image = dataset[idx][\"pixel_values\"]\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, multimask_output=False)\n",
    "# apply sigmoid\n",
    "sam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "sam_seg = (sam_seg_prob > 0.5).astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:42:08.515884200Z",
     "start_time": "2023-11-24T20:42:06.033460600Z"
    }
   },
   "id": "f3d8b6394171e4a0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "base_model_name = \"wanglab/medsam-vit-base\"\n",
    "processor = SamProcessor.from_pretrained(base_model_name)\n",
    "model = SamModel.from_pretrained(base_model_name)\n",
    "device = \"cuda\"\n",
    "idx = 3\n",
    "image = dataset[idx][\"pixel_values\"]\n",
    "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
    "prompt = get_bounding_box(ground_truth_mask)\n",
    "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, multimask_output=False)\n",
    "# apply sigmoid\n",
    "sam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "# convert soft mask to hard mask\n",
    "sam_seg_prob = sam_seg_prob.cpu().numpy().squeeze()\n",
    "medsam_seg = (sam_seg_prob > 0.5).astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:42:11.676721700Z",
     "start_time": "2023-11-24T20:42:09.482921900Z"
    }
   },
   "id": "c7fcd954774a5dcd"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "metric2 = metric.compute(\n",
    "    predictions=[ sam_seg ],\n",
    "    references=[ground_truth_mask],\n",
    "    num_labels=2,\n",
    "    ignore_index=255,\n",
    "    reduce_labels=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:44:53.515814900Z",
     "start_time": "2023-11-24T20:44:53.221559200Z"
    }
   },
   "id": "7dc58848ec6e1dcf"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "metric1 = metric.compute(\n",
    "    predictions=[finetune_seg],\n",
    "    references=[ground_truth_mask],\n",
    "    num_labels=2,\n",
    "    ignore_index=255,\n",
    "    reduce_labels=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:44:54.028201400Z",
     "start_time": "2023-11-24T20:44:53.730135200Z"
    }
   },
   "id": "88df284460886cd1"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "metric3 = metric.compute(\n",
    "    predictions=[ medsam_seg],\n",
    "    references=[ ground_truth_mask],\n",
    "    num_labels=2,\n",
    "    ignore_index=255,\n",
    "    reduce_labels=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:44:54.573930500Z",
     "start_time": "2023-11-24T20:44:54.281032900Z"
    }
   },
   "id": "d30aa453d5f477bd"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "{'mean_iou': 0.486114501953125,\n 'mean_accuracy': 0.5,\n 'overall_accuracy': 0.97222900390625,\n 'per_category_iou': array([0.972229, 0.      ]),\n 'per_category_accuracy': array([1., 0.])}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:45:09.639822600Z",
     "start_time": "2023-11-24T20:45:09.626590200Z"
    }
   },
   "id": "25dbfb701ba1cf6f"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "{'mean_iou': 0.5468189903686046,\n 'mean_accuracy': 0.864116953734946,\n 'overall_accuracy': 0.9039154052734375,\n 'per_category_iou': array([0.90167083, 0.19196715]),\n 'per_category_accuracy': array([0.90625589, 0.82197802])}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:45:10.502038900Z",
     "start_time": "2023-11-24T20:45:10.490896700Z"
    }
   },
   "id": "2749bdfa6049a867"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "{'mean_iou': 0.6032436680322589,\n 'mean_accuracy': 0.7210000731266974,\n 'overall_accuracy': 0.9603424072265625,\n 'per_category_iou': array([0.95982067, 0.24666667]),\n 'per_category_accuracy': array([0.97441773, 0.46758242])}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T20:45:11.397763800Z",
     "start_time": "2023-11-24T20:45:11.373569600Z"
    }
   },
   "id": "295f2e79d5c92c80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6119bd154aab6597"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
