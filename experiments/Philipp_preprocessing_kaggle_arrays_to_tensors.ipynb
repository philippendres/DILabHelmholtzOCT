{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from segment_anything.utils import transforms\n",
    "device = \"cuda\"\n",
    "import torch\n",
    "from segment_anything.modeling import sam\n",
    "from segment_anything import sam_model_registry"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:22:44.990421600Z",
     "start_time": "2023-11-22T19:22:44.987904200Z"
    }
   },
   "id": "4d01d03eee56999d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    image = np.expand_dims(img, axis=2)\n",
    "    image = np.repeat(255 - image, 3, axis=2)\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:12:30.661952500Z",
     "start_time": "2023-11-22T19:12:30.649936300Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open(\"..\\data\\Kaggle\\preprocessed\\\\amd_001.pkl\", 'rb') as file:\n",
    "    dict = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:13:31.887506200Z",
     "start_time": "2023-11-22T19:13:31.270364300Z"
    }
   },
   "id": "65aa773e8256cbce"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "image = dict['images'][30]['image']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:13:33.743085500Z",
     "start_time": "2023-11-22T19:13:33.730019500Z"
    }
   },
   "id": "9cf5ce6979dd93b9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "transformed_image = transform_image(image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:13:34.624300500Z",
     "start_time": "2023-11-22T19:13:34.599024Z"
    }
   },
   "id": "74c647f174dc97ff"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[ 67,  67,  67],\n        [ 67,  67,  67],\n        [ 69,  69,  69],\n        ...,\n        [ 31,  31,  31],\n        [ 34,  34,  34],\n        [ 32,  32,  32]],\n\n       [[ 56,  56,  56],\n        [ 57,  57,  57],\n        [ 57,  57,  57],\n        ...,\n        [ 21,  21,  21],\n        [ 24,  24,  24],\n        [ 24,  24,  24]],\n\n       [[ 56,  56,  56],\n        [ 55,  55,  55],\n        [ 53,  53,  53],\n        ...,\n        [ 24,  24,  24],\n        [ 25,  25,  25],\n        [ 26,  26,  26]],\n\n       ...,\n\n       [[238, 238, 238],\n        [178, 178, 178],\n        [233, 233, 233],\n        ...,\n        [223, 223, 223],\n        [255, 255, 255],\n        [184, 184, 184]],\n\n       [[187, 187, 187],\n        [201, 201, 201],\n        [202, 202, 202],\n        ...,\n        [194, 194, 194],\n        [255, 255, 255],\n        [168, 168, 168]],\n\n       [[237, 237, 237],\n        [203, 203, 203],\n        [255, 255, 255],\n        ...,\n        [207, 207, 207],\n        [196, 196, 196],\n        [171, 171, 171]]], dtype=uint8)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:13:37.804009800Z",
     "start_time": "2023-11-22T19:13:37.777958100Z"
    }
   },
   "id": "618199a123dde0d9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "transform = transforms.ResizeLongestSide(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:15:50.845671400Z",
     "start_time": "2023-11-22T19:15:50.819965500Z"
    }
   },
   "id": "8caccb63486ca3d6"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "image_transformed = transform.apply_image(transformed_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:16:37.941074500Z",
     "start_time": "2023-11-22T19:16:37.922005400Z"
    }
   },
   "id": "aa91fecc8bbb7a02"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "image_torched = torch.as_tensor(image_transformed, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:18:36.385430700Z",
     "start_time": "2023-11-22T19:18:36.213425500Z"
    }
   },
   "id": "f9338aaaacdff2b1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_checkpoint = \"../data/SAM_checkpoint/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "device = \"cuda\"\n",
    "sam_model = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam_model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:22:49.180202800Z",
     "start_time": "2023-11-22T19:22:47.982530100Z"
    }
   },
   "id": "fa67e401bc1c1214"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "sam_object = sam.Sam(sam_model.image_encoder,sam_model.prompt_encoder, sam_model.mask_decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:22:51.403696100Z",
     "start_time": "2023-11-22T19:22:51.401186800Z"
    }
   },
   "id": "a6a225391f5f204c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m image_preprocessed \u001B[38;5;241m=\u001B[39m \u001B[43msam_object\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_torched\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\DILab\\Lib\\site-packages\\segment_anything\\modeling\\sam.py:167\u001B[0m, in \u001B[0;36mSam.preprocess\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Normalize pixel values and pad to a square input.\"\"\"\u001B[39;00m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;66;03m# Normalize colors\u001B[39;00m\n\u001B[1;32m--> 167\u001B[0m x \u001B[38;5;241m=\u001B[39m (\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpixel_mean\u001B[49m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpixel_std\n\u001B[0;32m    169\u001B[0m \u001B[38;5;66;03m# Pad\u001B[39;00m\n\u001B[0;32m    170\u001B[0m h, w \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:]\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "image_preprocessed = sam_object.preprocess(image_torched)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:23:32.116Z",
     "start_time": "2023-11-22T19:23:31.608277600Z"
    }
   },
   "id": "431810b7cc556ef2"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([512, 1000, 3])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_torched.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:28:37.264164900Z",
     "start_time": "2023-11-22T19:28:37.251052700Z"
    }
   },
   "id": "b4055bff4f2f6256"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "input_image_torch = image_torched.permute(2, 0, 1).contiguous()[None, :, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:29:22.365534500Z",
     "start_time": "2023-11-22T19:29:22.339388Z"
    }
   },
   "id": "58f3b08395e8e14f"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 512, 1000])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image_torch.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:29:36.205236900Z",
     "start_time": "2023-11-22T19:29:36.191693100Z"
    }
   },
   "id": "f20b78190358133b"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Sam(\n  (image_encoder): ImageEncoderViT(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): MLPBlock(\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): Sequential(\n      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): LayerNorm2d()\n      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (3): LayerNorm2d()\n    )\n  )\n  (prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLPBlock(\n            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_object.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:31:37.235776400Z",
     "start_time": "2023-11-22T19:31:37.218509200Z"
    }
   },
   "id": "d062dd25c4505234"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "image_preprocessed = sam_object.preprocess(input_image_torch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:31:41.375762500Z",
     "start_time": "2023-11-22T19:31:41.327400400Z"
    }
   },
   "id": "d4f3018444dbdae"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[-0.9705, -0.9705, -0.9363,  ...,  0.0000,  0.0000,  0.0000],\n          [-1.1589, -1.1418, -1.1418,  ...,  0.0000,  0.0000,  0.0000],\n          [-1.1589, -1.1760, -1.2103,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n         [[-0.8627, -0.8627, -0.8277,  ...,  0.0000,  0.0000,  0.0000],\n          [-1.0553, -1.0378, -1.0378,  ...,  0.0000,  0.0000,  0.0000],\n          [-1.0553, -1.0728, -1.1078,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n\n         [[-0.6367, -0.6367, -0.6018,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.8284, -0.8110, -0.8110,  ...,  0.0000,  0.0000,  0.0000],\n          [-0.8284, -0.8458, -0.8807,  ...,  0.0000,  0.0000,  0.0000],\n          ...,\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n       device='cuda:0')"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_preprocessed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:31:58.418633100Z",
     "start_time": "2023-11-22T19:31:58.398334700Z"
    }
   },
   "id": "b7e551f0f6bfbbe6"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:32:24.818395900Z",
     "start_time": "2023-11-22T19:32:24.797834500Z"
    }
   },
   "id": "9f29d897aae69c7"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:32:30.202216Z",
     "start_time": "2023-11-22T19:32:30.179988200Z"
    }
   },
   "id": "b97a3a34a7ce04d7"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embedding = sam_model.image_encoder(image_preprocessed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:32:47.524912400Z",
     "start_time": "2023-11-22T19:32:46.630566200Z"
    }
   },
   "id": "5c3e47ca2b0eaeea"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "box = dict['images'][30]['bbox'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:34:30.253625900Z",
     "start_time": "2023-11-22T19:34:30.223597300Z"
    }
   },
   "id": "5acd25956018f741"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "torched_box = torch.as_tensor(box, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T21:44:20.788099500Z",
     "start_time": "2023-11-22T21:44:20.761229400Z"
    }
   },
   "id": "3ee94418b0e24cd5"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  0., 259.,  66., 859.], device='cuda:0', dtype=torch.float64)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torched_box"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T21:44:31.009814Z",
     "start_time": "2023-11-22T21:44:30.975114500Z"
    }
   },
   "id": "5479c290dc1d6e2f"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0., 259.,  66., 859.]], device='cuda:0')"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torched_box[None,:].float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T21:45:51.890009200Z",
     "start_time": "2023-11-22T21:45:51.855353Z"
    }
   },
   "id": "a9000dd559958f13"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0., 259.,  66., 859.]], device='cuda:0')"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform.apply_boxes_torch(torched_box, (512,1000))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T21:44:15.700613500Z",
     "start_time": "2023-11-22T21:44:09.385272Z"
    }
   },
   "id": "d8446054e8612aa9"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=transform.apply_boxes_torch(torched_box, (512,1000)),\n",
    "        masks=None,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T21:45:26.530311300Z",
     "start_time": "2023-11-22T21:45:26.489625300Z"
    }
   },
   "id": "d1697c1a4cdbb784"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  0., 259.,  66., 859.])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:34:32.473803900Z",
     "start_time": "2023-11-22T19:34:32.463778300Z"
    }
   },
   "id": "bbb9406027333bd2"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "    image_embeddings=image_embedding,\n",
    "    image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "    sparse_prompt_embeddings=sparse_embeddings,\n",
    "    dense_prompt_embeddings=dense_embeddings,\n",
    "    multimask_output=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:38:26.124822Z",
     "start_time": "2023-11-22T19:38:25.933419300Z"
    }
   },
   "id": "93773cef6aa60212"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 256, 256])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_res_masks.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:39:36.294087600Z",
     "start_time": "2023-11-22T19:39:36.264556800Z"
    }
   },
   "id": "21c5f9ccb2c20e55"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "upscaled_masks = sam_model.postprocess_masks(low_res_masks, (256,256), (512,1000)).to(device)\n",
    "\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "binary_mask = normalize(threshold(upscaled_masks, 0.0, 0)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:41:06.359450900Z",
     "start_time": "2023-11-22T19:41:06.259114300Z"
    }
   },
   "id": "ae155e1134ee883a"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 512, 1000])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_mask.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:41:15.723947300Z",
     "start_time": "2023-11-22T19:41:15.707810400Z"
    }
   },
   "id": "4c1e0370255bc89e"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "gt_binary_mask = dict['images'][30]['segmentations'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:42:05.371559300Z",
     "start_time": "2023-11-22T19:42:05.354949700Z"
    }
   },
   "id": "d3d0800f4ca5ac82"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_binary_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:42:19.187992200Z",
     "start_time": "2023-11-22T19:42:19.169772700Z"
    }
   },
   "id": "2ce94aafa738b803"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "torched_gt = torch.as_tensor(gt_binary_mask, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:43:12.624766400Z",
     "start_time": "2023-11-22T19:43:12.600573900Z"
    }
   },
   "id": "e2f3ee9fc6f11ab2"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 512, 1000])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torched_gt[None,None,:,:].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:43:43.785183600Z",
     "start_time": "2023-11-22T19:43:43.749714700Z"
    }
   },
   "id": "d2b3847d94018d63"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "gt = torched_gt[None,None,:,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:44:05.438767900Z",
     "start_time": "2023-11-22T19:44:05.418739800Z"
    }
   },
   "id": "926180f499ae3017"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "loss = loss_fn(binary_mask, gt.float())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:46:26.380436600Z",
     "start_time": "2023-11-22T19:46:26.355698700Z"
    }
   },
   "id": "de3e4d474bd0776a"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:46:27.595703200Z",
     "start_time": "2023-11-22T19:46:26.909342900Z"
    }
   },
   "id": "bfc16193e44b9b50"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T19:46:15.331194300Z",
     "start_time": "2023-11-22T19:46:15.313558700Z"
    }
   },
   "id": "d2b4256e484fe514"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c2aa90a3422401dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
