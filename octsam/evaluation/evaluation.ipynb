{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader as TorchDataset\n",
    "from torch.optim import Adam\n",
    "import wandb\n",
    "import monai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "from transformers import SamProcessor, SamModel\n",
    "from statistics import mean\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from scipy.ndimage import label\n",
    "import evaluate\n",
    "# Model info\n",
    "# base_models = [\"facebook/sam-vit-base\", \"facebook/sam-vit-huge\", \"facebook/sam-vit-large\", \"wanglab/medsam-vit-base\"]\n",
    "base_model = \"facebook/sam-vit-base\"\n",
    "checkpoint_path = \"/vol/data/models/custom_24-01-24_23.49.46\"\n",
    "dataset_path = \"/vol/data/datasets/processed/custom/default_preprocessed_at_24-01-10_13.41.28\"\n",
    "pseudocolor = \"grayscale\"\n",
    "prompt_type = \"bboxes\"\n",
    "\n",
    "\n",
    "###\n",
    "def evaluate_metrics(model, dataset, config, processor):\n",
    "    model.eval()\n",
    "    metric = evaluate.load(\"mean_iou\")\n",
    "    segmentations = []\n",
    "    ground_truths = []\n",
    "    for i in tqdm(range(3)):#len(dataset))):\n",
    "        with torch.no_grad():\n",
    "            if (config[\"prompt_type\"]==\"points\"):\n",
    "                image, points, gt_masks, mask_values = dataset[i]\n",
    "                inputs = processor(image, input_points= [points], return_tensors=\"pt\")\n",
    "            else:\n",
    "                image, bboxes, gt_masks, mask_values = dataset[i]\n",
    "                inputs = processor(image, input_boxes=[bboxes], return_tensors=\"pt\")\n",
    "            outputs= model(**inputs, multimask_output=False)\n",
    "            masks = torch.zeros(1, 14, 496, 512)\n",
    "            masks = F.interpolate(outputs.pred_masks.squeeze(2), (1024,1024), mode=\"bilinear\", align_corners=False)\n",
    "            masks = masks[..., : inputs[\"reshaped_input_sizes\"][0,0], : inputs[\"reshaped_input_sizes\"][0,1]]\n",
    "            masks = F.interpolate(masks, (inputs[\"original_sizes\"][0,0],inputs[\"original_sizes\"][0,1]), mode=\"bilinear\", align_corners=False)\n",
    "            masks = torch.sigmoid(masks).squeeze().numpy()\n",
    "            binary_masks = (masks > 0.5).astype(np.uint8)\n",
    "            for c in range(len(mask_values)):\n",
    "                if mask_values[c] == 0 and c > 0:\n",
    "                    break\n",
    "                segmentations.append(binary_masks[c]* (mask_values[c]+1))\n",
    "                ground_truths.append(gt_masks[c]* (mask_values[c]+1))\n",
    "        \n",
    "    metric_output = metric.compute(\n",
    "        predictions=segmentations,\n",
    "        references=ground_truths,\n",
    "        ignore_index=255,\n",
    "        num_labels=14,\n",
    "        reduce_labels=True,\n",
    "    )\n",
    "    print(metric_output)\n",
    "    f = open(config[\"results_path\"], \"w\")\n",
    "    f.write(str(metric_output))\n",
    "    f.close()\n",
    "###\n",
    "\n",
    "class SAMDataset(TorchDataset):\n",
    "    def __init__(self, dataset, processor, config):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_bboxes_and_gt_masks(self, ground_truth_mask):\n",
    "        # get bounding boxes from mask\n",
    "        structure = np.ones((3, 3), dtype=np.int32)\n",
    "        bboxes, gt_masks = [],[]\n",
    "        mask_values= np.unique(ground_truth_mask)\n",
    "        final_mask_values = []\n",
    "        #Comment for background prediction\n",
    "        #mask_values = mask_values[1:]\n",
    "        for v in mask_values: \n",
    "            binary_gt_mask = np.where(ground_truth_mask == v, 1.0, 0.0)\n",
    "            labeled_gt_mask, ncomponents = label(binary_gt_mask, structure)\n",
    "            for c in range(ncomponents):\n",
    "                final_mask_values.append(v)\n",
    "                x_indices, y_indices = np.where(labeled_gt_mask== c+1)\n",
    "                x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "                y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "                # add perturbation to bounding box coordinates\n",
    "                H, W = ground_truth_mask.shape\n",
    "                x_min = max(0, x_min + np.random.randint(-10, 10))\n",
    "                x_max = min(W, x_max + np.random.randint(-10, 10))\n",
    "                y_min = max(0, y_min + np.random.randint(-10, 10))\n",
    "                y_max = min(H, y_max + np.random.randint(-10, 10))\n",
    "                bbox = [x_min, y_min, x_max, y_max]\n",
    "                bboxes.append(bbox)\n",
    "                gt_mask = np.where(labeled_gt_mask== c+1, 1.0, 0.0)\n",
    "                gt_masks.append(gt_mask)\n",
    "        return bboxes, gt_masks, final_mask_values\n",
    "\n",
    "    def get_points_and_gt_masks(self, ground_truth_mask):\n",
    "        structure = np.ones((3, 3), dtype=np.int32)\n",
    "        points, gt_masks = [],[]\n",
    "        mask_values= np.unique(ground_truth_mask)\n",
    "        final_mask_values = []\n",
    "        #Comment for background prediction\n",
    "        #mask_values= mask_values[1:]\n",
    "        for v in mask_values: \n",
    "            binary_gt_mask = np.where(ground_truth_mask == v, 1.0, 0.0)\n",
    "            labeled_gt_mask, ncomponents = label(binary_gt_mask, structure)\n",
    "            for c in range(ncomponents):\n",
    "                final_mask_values.append(v)\n",
    "                x_indices, y_indices = np.where(labeled_gt_mask== c+1)\n",
    "                rand_idx = random.randrange(0, len(x_indices))\n",
    "                points.append([[x_indices[rand_idx], y_indices[rand_idx]]])\n",
    "                gt_mask = np.where(labeled_gt_mask== c+1, 1.0, 0.0)\n",
    "                gt_masks.append(gt_mask)\n",
    "        return points, gt_masks, final_mask_values\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = np.array(item[\"image\"])\n",
    "        if (self.config[\"pseudocolor\"] != None):\n",
    "            image = cv2.applyColorMap(image[:, :, 0], self.config[\"pseudocolor\"])\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        if (self.config[\"prompt_type\"]==\"points\"):\n",
    "            points, gt_masks, mask_values= self.get_points_and_gt_masks(ground_truth_mask)\n",
    "            return [image, points, gt_masks, mask_values]\n",
    "        else:\n",
    "            bboxes, gt_masks, mask_values= self.get_bboxes_and_gt_masks(ground_truth_mask)\n",
    "            return [image, bboxes, gt_masks, mask_values]\n",
    "\n",
    "#Colormap\n",
    "OCV_COLORMAPS = {\n",
    "    \"Autumn\": cv2.COLORMAP_AUTUMN, \n",
    "    \"Bone\": cv2.COLORMAP_BONE,\n",
    "    \"Cividis\": cv2.COLORMAP_CIVIDIS, \n",
    "    \"Cool\": cv2.COLORMAP_COOL, \n",
    "    \"Deepgreen\": cv2.COLORMAP_DEEPGREEN,\n",
    "    \"Hot\": cv2.COLORMAP_HOT,\n",
    "    \"HSV\": cv2.COLORMAP_HSV,\n",
    "    \"Inferno\": cv2.COLORMAP_INFERNO,\n",
    "    \"Jet\": cv2.COLORMAP_JET,\n",
    "    \"Magma\": cv2.COLORMAP_MAGMA,\n",
    "    \"Ocean\": cv2.COLORMAP_OCEAN,\n",
    "    \"Parula\": cv2.COLORMAP_PARULA,\n",
    "    \"Pink\": cv2.COLORMAP_PINK,\n",
    "    \"Plasma\": cv2.COLORMAP_PLASMA,\n",
    "    \"Rainbow\": cv2.COLORMAP_RAINBOW,\n",
    "    \"Viridis\": cv2.COLORMAP_VIRIDIS,\n",
    "    \"Winter\": cv2.COLORMAP_WINTER,\n",
    "    \"Spring\": cv2.COLORMAP_SPRING,\n",
    "    \"Summer\": cv2.COLORMAP_SUMMER,\n",
    "    \"Twilight shifted\": cv2.COLORMAP_TWILIGHT_SHIFTED,\n",
    "    \"Twilight\": cv2.COLORMAP_TWILIGHT,\n",
    "    \"Turbo\": cv2.COLORMAP_TURBO,\n",
    "    \"grayscale\": None\n",
    "}\n",
    "# mask_dict\n",
    "mask_dict = (\n",
    "    \"background\",\n",
    "    \"epiretinal membrane\",\n",
    "    \"neurosensory retina\",\n",
    "    \"intraretinal fluid\",\n",
    "    \"subretinal fluid\",\n",
    "    \"subretinal hyperreflective material\",\n",
    "    \"retinal pigment epithelium\",\n",
    "    \"pigment epithelial detachment\",\n",
    "    \"posterior hyaloid membrane\",\n",
    "    \"choroid border\",\n",
    "    \"imaging artifacts\",\n",
    "    \"fibrosis\",\n",
    "    \"vitreous body\",\n",
    "    \"image padding\" \n",
    ")\n",
    "\n",
    "processor = SamProcessor.from_pretrained(base_model)\n",
    "model = SamModel.from_pretrained(base_model)\n",
    "model.load_state_dict(torch.load(checkpoint_path +\".pt\"))\n",
    "dataset = datasets.load_from_disk(dataset_path)[\"test\"]\n",
    "config ={\n",
    "    \"pseudocolor\": OCV_COLORMAPS[pseudocolor],\n",
    "    \"prompt_type\": prompt_type,\n",
    "    \"mask_dict\": mask_dict,\n",
    "    \"results_path\": checkpoint_path + \".txt\"\n",
    "}\n",
    "dataset = SAMDataset(dataset=dataset, processor=processor, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "segmentations = []\n",
    "ground_truths = []\n",
    "for i in range(14):\n",
    "    segmentations.append([])\n",
    "    ground_truths.append([])\n",
    "for i in tqdm(range(1)):#len(dataset))):\n",
    "    with torch.no_grad():\n",
    "        if (config[\"prompt_type\"]==\"points\"):\n",
    "            image, points, gt_masks, mask_values = dataset[i]\n",
    "            inputs = processor(image, input_points= [points], return_tensors=\"pt\")\n",
    "        else:\n",
    "            image, bboxes, gt_masks, mask_values = dataset[i]\n",
    "            inputs = processor(image, input_boxes=[bboxes], return_tensors=\"pt\")\n",
    "        outputs= model(**inputs, multimask_output=False)\n",
    "        masks = torch.zeros(1, 14, 496, 512)\n",
    "        masks = F.interpolate(outputs.pred_masks.squeeze(2), (1024,1024), mode=\"bilinear\", align_corners=False)\n",
    "        masks = masks[..., : inputs[\"reshaped_input_sizes\"][0,0], : inputs[\"reshaped_input_sizes\"][0,1]]\n",
    "        masks = F.interpolate(masks, (inputs[\"original_sizes\"][0,0],inputs[\"original_sizes\"][0,1]), mode=\"bilinear\", align_corners=False)\n",
    "        masks = torch.sigmoid(masks).squeeze().numpy()\n",
    "        binary_masks = (masks > 0.5).astype(np.uint8)\n",
    "        for c in range(len(mask_values)):\n",
    "            if mask_values[c] == 0 and c > 0:\n",
    "                break\n",
    "            segmentations[mask_values[c]].append(binary_masks[c])\n",
    "            ground_truths[mask_values[c]].append(gt_masks[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_output = metric.compute(\n",
    "    predictions=segmentations[2],\n",
    "    references=ground_truths[2],\n",
    "    ignore_index=255,\n",
    "    num_labels=1,\n",
    "    reduce_labels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_iou': 0.8037700037802419,\n",
       " 'mean_accuracy': 0.8037700037802419,\n",
       " 'overall_accuracy': 0.8037700037802419,\n",
       " 'per_category_iou': array([0.80377]),\n",
       " 'per_category_accuracy': array([0.80377])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in segmentations:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_iou': 0.17971943715306166, 'mean_accuracy': 0.17971943715306166, 'overall_accuracy': 0.17971943715306166, 'per_category_iou': array([0.17971944]), 'per_category_accuracy': array([0.17971944])}\n",
      "{'mean_iou': 1.0, 'mean_accuracy': 1.0, 'overall_accuracy': 1.0, 'per_category_iou': array([1.]), 'per_category_accuracy': array([1.])}\n",
      "{'mean_iou': 1.0, 'mean_accuracy': 1.0, 'overall_accuracy': 1.0, 'per_category_iou': array([1.]), 'per_category_accuracy': array([1.])}\n",
      "{'mean_iou': nan, 'mean_accuracy': nan, 'overall_accuracy': nan, 'per_category_iou': array([nan]), 'per_category_accuracy': array([nan])}\n",
      "{'mean_iou': nan, 'mean_accuracy': nan, 'overall_accuracy': nan, 'per_category_iou': array([nan]), 'per_category_accuracy': array([nan])}\n",
      "{'mean_iou': nan, 'mean_accuracy': nan, 'overall_accuracy': nan, 'per_category_iou': array([nan]), 'per_category_accuracy': array([nan])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:258: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  all_acc = total_area_intersect.sum() / total_area_label.sum()\n",
      "/home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "/home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n",
      "/home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:262: RuntimeWarning: Mean of empty slice\n",
      "  metrics[\"mean_iou\"] = np.nanmean(iou)\n",
      "/home/ubuntu/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:263: RuntimeWarning: Mean of empty slice\n",
      "  metrics[\"mean_accuracy\"] = np.nanmean(acc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_iou': 1.0, 'mean_accuracy': 1.0, 'overall_accuracy': 1.0, 'per_category_iou': array([1.]), 'per_category_accuracy': array([1.])}\n",
      "{'mean_iou': 1.0, 'mean_accuracy': 1.0, 'overall_accuracy': 1.0, 'per_category_iou': array([1.]), 'per_category_accuracy': array([1.])}\n",
      "{'mean_iou': nan, 'mean_accuracy': nan, 'overall_accuracy': nan, 'per_category_iou': array([nan]), 'per_category_accuracy': array([nan])}\n",
      "{'mean_iou': nan, 'mean_accuracy': nan, 'overall_accuracy': nan, 'per_category_iou': array([nan]), 'per_category_accuracy': array([nan])}\n",
      "{'mean_iou': 0.9261353957275732, 'mean_accuracy': 0.9261353957275732, 'overall_accuracy': 0.9261353957275732, 'per_category_iou': array([0.9261354]), 'per_category_accuracy': array([0.9261354])}\n",
      "{'mean_iou': nan, 'mean_accuracy': nan, 'overall_accuracy': nan, 'per_category_iou': array([nan]), 'per_category_accuracy': array([nan])}\n",
      "{'mean_iou': 0.4174541164423965, 'mean_accuracy': 0.4174541164423965, 'overall_accuracy': 0.4174541164423965, 'per_category_iou': array([0.41745412]), 'per_category_accuracy': array([0.41745412])}\n",
      "{'mean_iou': 0.853696906135291, 'mean_accuracy': 0.853696906135291, 'overall_accuracy': 0.853696906135291, 'per_category_iou': array([0.85369691]), 'per_category_accuracy': array([0.85369691])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(metric.compute(\n",
    "        predictions=segmentations[i],\n",
    "        references=ground_truths[i],\n",
    "        ignore_index=255,\n",
    "        num_labels=1,\n",
    "        reduce_labels=True,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12924729285714287"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.63437679+0.00345155+0.01173586+0.01183259+0.02157136+0.13978959+0.00600943+0.14160961+0.01343778+0.02645307+0.07045111+0.0290135+0.30911137+0.39061849) /14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_iou': 0.8037700037802419,\n",
       " 'mean_accuracy': 0.8037700037802419,\n",
       " 'overall_accuracy': 0.8037700037802419,\n",
       " 'per_category_iou': array([0.80377]),\n",
       " 'per_category_accuracy': array([0.80377])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8037700037802419"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_output['per_category_accuracy'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DILab3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
