{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application for fine-tuned SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#backend\n",
    "import torch\n",
    "from transformers import SamModel, SamProcessor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "checkpoint_path = \"/vol/data/models/custom5e-05 lr,1e-04 wd,2 bs, diceCE loss, grayscale, 24-02-23_17.35.30_24-02-23_17.35.30\"\n",
    "model.load_state_dict(torch.load(checkpoint_path +\".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img, pixel, prompt_type):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if (prompt_type==\"points\"):\n",
    "            inputs = processor(img, input_points= [[pixel]], return_tensors=\"pt\").to(device)\n",
    "        else:\n",
    "            inputs = processor(img, input_boxes=[[pixel]], return_tensors=\"pt\").to(device)\n",
    "        outputs= model(**inputs, multimask_output=False)\n",
    "        masks = F.interpolate(outputs.pred_masks.squeeze(2), (1024,1024), mode=\"bilinear\", align_corners=False)\n",
    "        masks = masks[..., : inputs[\"reshaped_input_sizes\"][0,0], : inputs[\"reshaped_input_sizes\"][0,1]]\n",
    "        masks = F.interpolate(masks, (inputs[\"original_sizes\"][0,0],inputs[\"original_sizes\"][0,1]), mode=\"bilinear\", align_corners=False)\n",
    "        masks = torch.sigmoid(masks).cpu().squeeze().numpy()\n",
    "        binary_masks = (masks > 0.5).astype(np.uint8)\n",
    "    return binary_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "Running on public URL: https://79716390a443ca3b95.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://79716390a443ca3b95.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#application points\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(label=\"Input\")\n",
    "        img_output = gr.AnnotatedImage(\n",
    "            color_map={\"red\": \"#ff0000\"}\n",
    "        )\n",
    "\n",
    "    def get_select_coords(img, evt: gr.SelectData):\n",
    "        pixel = evt.index\n",
    "        mask = inference(img, pixel, \"points\")\n",
    "        return (img, [(mask, \"red\")])\n",
    "    \n",
    "    input_img.select(get_select_coords, input_img, img_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "Running on public URL: https://11396e4f3f1e6be80e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://11396e4f3f1e6be80e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 49, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2103, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 823, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_326251/2392467796.py\", line 22, in get_select_coords\n",
      "    mask = inference(img, pixel, \"bboxes\", True)\n",
      "TypeError: inference() takes 3 positional arguments but 4 were given\n",
      "Traceback (most recent call last):\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 49, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2103, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 823, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/vol/data/miniconda3/envs/DILab3.10/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_326251/2392467796.py\", line 22, in get_select_coords\n",
      "    mask = inference(img, pixel, \"bboxes\", True)\n",
      "TypeError: inference() takes 3 positional arguments but 4 were given\n"
     ]
    }
   ],
   "source": [
    "#application bboxes\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "previous_point = None\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(label=\"Input\")\n",
    "        img_output = gr.AnnotatedImage(\n",
    "            color_map={\"mask\": \"#ff0000\", \"box\": \"#00ff00\", \"first corner\": \"#00ff00\"}\n",
    "        )\n",
    "\n",
    "    def get_select_coords(img, evt: gr.SelectData):\n",
    "        global previous_point\n",
    "        if previous_point:\n",
    "            pixel = previous_point+evt.index\n",
    "            mask = inference(img, pixel, \"bboxes\")\n",
    "            previous_point = None\n",
    "            return (img, [(pixel,\"box\"),(mask, \"mask\")])\n",
    "        else:\n",
    "            previous_point = evt.index\n",
    "            mask = np.zeros(img.shape[:2])\n",
    "            mask[previous_point[1]-1:previous_point[1]+2, previous_point[0]-1:previous_point[0]+2] = 1\n",
    "            return (img, [(mask, \"first corner\")])\n",
    "    \n",
    "    input_img.select(get_select_coords, input_img, img_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of SAM and fine-tuned SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backend\n",
    "import torch\n",
    "from transformers import SamModel, SamProcessor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n",
    "checkpoint_path = \"/vol/data/models/custom5e-05 lr,1e-04 wd,2 bs, diceCE loss, grayscale, 24-02-23_17.35.30_24-02-23_17.35.30\"\n",
    "model.load_state_dict(torch.load(checkpoint_path +\".pt\"))\n",
    "\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img, pixel, prompt_type, fine_tuned):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if (prompt_type==\"points\"):\n",
    "            inputs = processor(img, input_points= [[pixel]], return_tensors=\"pt\").to(device)\n",
    "        else:\n",
    "            inputs = processor(img, input_boxes=[[pixel]], return_tensors=\"pt\").to(device)\n",
    "        if (fine_tuned):\n",
    "            outputs= model(**inputs, multimask_output=False)\n",
    "        else:\n",
    "            outputs= sam_model(**inputs, multimask_output=False)\n",
    "        masks = F.interpolate(outputs.pred_masks.squeeze(2), (1024,1024), mode=\"bilinear\", align_corners=False)\n",
    "        masks = masks[..., : inputs[\"reshaped_input_sizes\"][0,0], : inputs[\"reshaped_input_sizes\"][0,1]]\n",
    "        masks = F.interpolate(masks, (inputs[\"original_sizes\"][0,0],inputs[\"original_sizes\"][0,1]), mode=\"bilinear\", align_corners=False)\n",
    "        masks = torch.sigmoid(masks).cpu().squeeze().numpy()\n",
    "        binary_masks = (masks > 0.5).astype(np.uint8)\n",
    "    return binary_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://e58955382a71b0ee72.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e58955382a71b0ee72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#application bboxes\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "previous_point = None\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(label=\"Input\")\n",
    "        img_output = gr.AnnotatedImage(\n",
    "            label=\"fine-tuned SAM\",\n",
    "            color_map={\"mask\": \"#ff0000\", \"box\": \"#00ff00\", \"first corner\": \"#00ff00\"}\n",
    "        )\n",
    "        img_sam = gr.AnnotatedImage(\n",
    "            label =\"default SAM\",\n",
    "            color_map={\"mask\": \"#ff0000\", \"box\": \"#00ff00\", \"first corner\": \"#00ff00\"}\n",
    "        )\n",
    "\n",
    "    def get_select_coords(img, evt: gr.SelectData):\n",
    "        global previous_point\n",
    "        if previous_point:\n",
    "            pixel = previous_point+evt.index\n",
    "            mask = inference(img, pixel, \"bboxes\", True)\n",
    "            sam_mask = inference(img, pixel, \"bboxes\", False)\n",
    "            previous_point = None\n",
    "            return ((img, [(pixel,\"box\"),(mask, \"mask\")]), (img, [(pixel,\"box\"),(sam_mask, \"mask\")]))\n",
    "        else:\n",
    "            previous_point = evt.index\n",
    "            mask = np.zeros(img.shape[:2])\n",
    "            mask[previous_point[1]-1:previous_point[1]+2, previous_point[0]-1:previous_point[0]+2] = 1\n",
    "            return ((img, [(mask, \"first corner\")]),(img, [(mask, \"first corner\")]))\n",
    "    \n",
    "    input_img.select(get_select_coords, input_img, [img_output, img_sam])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DILab3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
